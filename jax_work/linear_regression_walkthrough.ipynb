{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.1.4-cp39-cp39-macosx_10_9_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.1.4-cp39-cp39-macosx_10_9_x86_64.whl (11.8 MB)\n",
      "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Using cached tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.1.4 pytz-2023.3.post1 tzdata-2023.4\n",
      "Collecting plotnine\n",
      "  Using cached plotnine-0.12.4-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting matplotlib>=3.6.0 (from plotnine)\n",
      "  Using cached matplotlib-3.8.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting mizani<0.10.0,>0.9.0 (from plotnine)\n",
      "  Using cached mizani-0.9.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from plotnine) (1.26.3)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from plotnine) (2.1.4)\n",
      "Collecting patsy>=0.5.1 (from plotnine)\n",
      "  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from plotnine) (1.11.4)\n",
      "Collecting statsmodels>=0.14.0 (from plotnine)\n",
      "  Using cached statsmodels-0.14.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.6.0->plotnine)\n",
      "  Using cached contourpy-1.2.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.6.0->plotnine)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.6.0->plotnine)\n",
      "  Using cached fonttools-4.47.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (157 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.6.0->plotnine)\n",
      "  Using cached kiwisolver-1.4.5-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from matplotlib>=3.6.0->plotnine) (23.2)\n",
      "Collecting pillow>=8 (from matplotlib>=3.6.0->plotnine)\n",
      "  Using cached pillow-10.2.0-cp39-cp39-macosx_10_10_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.6.0->plotnine)\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from matplotlib>=3.6.0->plotnine) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from matplotlib>=3.6.0->plotnine) (6.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from pandas>=1.5.0->plotnine) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from pandas>=1.5.0->plotnine) (2023.4)\n",
      "Requirement already satisfied: six in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from patsy>=0.5.1->plotnine) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/mm/Documents/Data Science/Blog Posts/venv9/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.6.0->plotnine) (3.17.0)\n",
      "Using cached plotnine-0.12.4-py3-none-any.whl (1.3 MB)\n",
      "Using cached matplotlib-3.8.2-cp39-cp39-macosx_10_12_x86_64.whl (7.6 MB)\n",
      "Using cached mizani-0.9.3-py3-none-any.whl (73 kB)\n",
      "Using cached patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Using cached statsmodels-0.14.1-cp39-cp39-macosx_10_9_x86_64.whl (10.6 MB)\n",
      "Using cached contourpy-1.2.0-cp39-cp39-macosx_10_9_x86_64.whl (257 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.47.2-cp39-cp39-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "Using cached kiwisolver-1.4.5-cp39-cp39-macosx_10_9_x86_64.whl (68 kB)\n",
      "Using cached pillow-10.2.0-cp39-cp39-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: pyparsing, pillow, patsy, kiwisolver, fonttools, cycler, contourpy, matplotlib, statsmodels, mizani, plotnine\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.47.2 kiwisolver-1.4.5 matplotlib-3.8.2 mizani-0.9.3 patsy-0.5.6 pillow-10.2.0 plotnine-0.12.4 pyparsing-3.1.1 statsmodels-0.14.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade -q pip jax jaxlib\n",
    "# !pip install --upgrade -q git+https://github.com/google/flax.git\n",
    "# !pip install pandas\n",
    "# !pip install plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence\n",
    "from jax import random, numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': (5,), 'kernel': (10, 5)}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Dense(features = 5)\n",
    "## A linear transformation applied over the last dimension of the input\n",
    "key1, key2 = random.split(random.key(0))\n",
    "x = random.normal(key1, (10, ))\n",
    "params = model.init(key2, x)\n",
    "## Initializes a module method with variables and returns modified variables.\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params)\n",
    "## Maps a multi-input function over pytree args to produce a new pytree\n",
    "## params is the old pytree, so this is basically applying shape() to each key in the param dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.3721193 ,  0.61131495,  0.6442836 ,  2.2192965 , -1.1271116 ],      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x)\n",
    "## Applies a module method to variables and returns output and modified variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x shape: (20, 10); y shape: (20, 5)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "key = random.key(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim, ))\n",
    "true_params = flax.core.freeze({'params': {'bias': b, 'kernel': W}})\n",
    "\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise, (n_samples, y_dim))\n",
    "f'x shape: {x_samples.shape}; y shape: {y_samples.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "    # Define the squared loss for a single pair (x, y)\n",
    "    def squared_error(x, y):\n",
    "        pred = model.apply(params, x)\n",
    "        return jnp.inner(y - pred, y - pred) / 2.0\n",
    "    # Vectorize the previous to compute the average of the loss on all samples.\n",
    "    return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W, b: 0.02363979071378708\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3\n",
    "print(f'Loss for \"true\" W, b: {mse(true_params, x_samples, y_samples)}')\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "## value_and_grad returns a function that can be called on data that will return the value and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 35.343875885009766\n",
      "Loss step 10: 0.5143468976020813\n",
      "Loss step 20: 0.11384159326553345\n",
      "Loss step 30: 0.0393267385661602\n",
      "Loss step 40: 0.01991621032357216\n",
      "Loss step 50: 0.014209136366844177\n",
      "Loss step 60: 0.012425652705132961\n",
      "Loss step 70: 0.011850389651954174\n",
      "Loss step 80: 0.011661785654723644\n",
      "Loss step 90: 0.011599408462643623\n",
      "Loss step 100: 0.011578695848584175\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "    def move_params(params, grads):\n",
    "        params - learning_rate * grads\n",
    "        ## if g is positive then move away from current params\n",
    "        ## if g is negative then move towards current params\n",
    "        ## we want to minimize mse, so reduce the value = negative gradient\n",
    "        return move_params\n",
    "    \n",
    "    new_params = jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "    return new_params\n",
    "\n",
    "for i in range(101):\n",
    "    # Perform one gradient update\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    params = update_params(params, learning_rate, grads)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Loss step {i}: {loss_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 0.011577627621591091\n",
      "Loss step 10: 0.2614315450191498\n",
      "Loss step 20: 0.0767502710223198\n",
      "Loss step 30: 0.03644055128097534\n",
      "Loss step 40: 0.022012805566191673\n",
      "Loss step 50: 0.016178598627448082\n",
      "Loss step 60: 0.01300280075520277\n",
      "Loss step 70: 0.01202614326030016\n",
      "Loss step 80: 0.01176451425999403\n",
      "Loss step 90: 0.011646044440567493\n",
      "Loss step 100: 0.011585528962314129\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "tx = optax.adam(learning_rate = learning_rate)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "for i in range(101):\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Loss step {i}: {loss_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': Array([-1.4555768 , -2.0277991 ,  2.0790975 ,  1.2186145 , -0.99809754],      dtype=float32),\n",
       "  'kernel': Array([[ 1.0098814 ,  0.18934374,  0.04454996, -0.9280221 ,  0.3478402 ],\n",
       "         [ 1.7298453 ,  0.9879368 ,  1.1640464 ,  1.1006076 , -0.10653935],\n",
       "         [-1.2029463 ,  0.28635228,  1.4155979 ,  0.11870951, -1.3141483 ],\n",
       "         [-1.1941489 , -0.18958491,  0.03413862,  1.3169426 ,  0.0806038 ],\n",
       "         [ 0.1385241 ,  1.3713038 , -1.3187183 ,  0.53152674, -2.2404997 ],\n",
       "         [ 0.56294024,  0.8122311 ,  0.3175201 ,  0.53455096,  0.9050039 ],\n",
       "         [-0.37926027,  1.7410393 ,  1.0790287 , -0.5039833 ,  0.9283062 ],\n",
       "         [ 0.9706492 , -1.3153403 ,  0.33681503,  0.8099344 , -1.2018458 ],\n",
       "         [ 1.0194312 , -0.6202479 ,  1.0818833 , -1.838974  , -0.45805007],\n",
       "         [-0.6436537 ,  0.45666698, -1.1329137 , -0.6853864 ,  0.16829035]],      dtype=float32)}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this transformation we took a $20x10$ matrix, multiplied it by a $10x5$ matrix to get a $20x5$ result. Then we took the differnce between this prediction matrix and the actual values. Then we took an inner product of these errors to get a squared error, by row, but vectorized with vmap to work across our full X dimension. So this would return a simple (20, 1) array. Then we took a mean of each observation's errors (across dimension 0). to get the mean MSE. My question is why can't we use matrix multiplication. Our errors are by row, if we transpose the matrix we get them by column, a $5x20$ times a $20x5$. this gives us a 5x5? The diagonal elements are what we want, but the off-diagonal elements aren't. No, we need by observation, which is $20x5$ by $5x20$ to get the squared error, but this produces a ton of off diagonal elements we don't need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.01159014, dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(params, x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.2043007e-06, dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.mean(jnp.inner(y_samples - model.apply(params, x_samples), y_samples - model.apply(params, x_samples)) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.48579866, -4.074929  , -2.0214841 , -0.20061362, -0.59897006],      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.48579878, -4.0749288 , -2.0214846 , -0.20061398, -0.59897006],      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x_samples)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.02080444, 0.00846512, 0.01537622, 0.0055256 , 0.0069695 ,\n",
       "       0.00957341, 0.01018183, 0.009361  , 0.01557972, 0.01286128,\n",
       "       0.00875765, 0.00492677, 0.00321356, 0.02069209, 0.02168619,\n",
       "       0.00257889, 0.00533569, 0.0045466 , 0.01921797, 0.02614925],      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = y_samples - model.apply(params, x_samples)\n",
    "jnp.diag(jnp.dot(errors, errors.T) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.01159014, dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(params, x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.02080444, 0.00846512, 0.01537622, 0.0055256 , 0.0069695 ,\n",
       "       0.00957341, 0.01018183, 0.009361  , 0.01557972, 0.01286128,\n",
       "       0.00875765, 0.00492677, 0.00321356, 0.02069209, 0.02168619,\n",
       "       0.00257889, 0.00533569, 0.0045466 , 0.01921797, 0.02614925],      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def mse2(params, x_batched, y_batched):\n",
    "    # Define the squared loss for a single pair (x, y)\n",
    "    def squared_error(x, y):\n",
    "        pred = model.apply(params, x)\n",
    "        return jnp.inner(y - pred, y - pred) / 2.0\n",
    "    # Vectorize the previous to compute the average of the loss on all samples.\n",
    "    return jax.vmap(squared_error)(x_batched, y_batched)\n",
    "\n",
    "mse2(params, x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so things to do to take this to our problem:\n",
    "\n",
    "1. Get our X and Y dimensions\n",
    "    [20, 60] => [20, 90]\n",
    "2. Build up our internal layers\n",
    "3. Compute our loss function \n",
    "  + squared loss: inner product on the errors\n",
    "  + negative log likelihood: apply the function to each observation, take the mean\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "`optax.GradientTransformation` aka `tx = optax.adam(...)`\n",
    "* Defined as a pair of pure functions (`init` & `update`)\n",
    "* Each time a gradient transformation is applied a new state is computed and returned\n",
    "* init with an example of the model params whose gradients will be transformed and get a corresponding pytree containing the initial value for the optimizer state\n",
    "* update with the new gradients and the old state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.07298189, 0.08691938, 0.08723002],\n",
       "       [0.02081857, 0.01866242, 0.05502256]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inits = nn.initializers.uniform(.1)\n",
    "inits(jax.random.PRNGKey(42), (2, 3), jnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Blog Posts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
